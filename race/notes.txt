In RadixIPLookup::add_route, the code is pretty tightly linked. We
first find a candidate key in the routing table vector, with 

      int found = (_vfree < 0 ? _v.size() : _vfree);

If concurrent threads access the above line, then they need to get
exclusive access on _vfree and _v. The unfortunate part is that, they
can't release the lock as soon as this statement is over as _v and
_vfree are maninpulated at a later stage.

    if (last_key && old_route)
	*old_route = _v[last_key - 1];
    if (last_key && !set)
	return -EEXIST;

    if (found == _v.size())
	_v.push_back(route);
    else {
	_vfree = _v[found].extra;
	_v[found] = route;
    }
    _v[found].extra = -1;

    if (last_key) {
	_v[last_key - 1].extra = _vfree;
	_vfree = last_key - 1;
    }

The problem arises because of "last_key." We can't obtain last_key
before change has been called. Without the "last_key" we don't know if
we need to add a route into the table or return an error. We can say
that if "set" is true, we will update the table irrespective of the
last key value.

We could create separate functions which handle the two cases -- set
is true or false -- and reorder the way the statements are executed so
that concurrent updaters are not blocked on the same region. But even
then, the issue remains in the case when set is false -- which is the
default behavior.

An alternative design is to have "d" vectors for the d levels. Instead
of acquiring a lock at a global level, we acquire the lock at a
particular depth.


In make_radix, we might need a memory barrier after the memset,
otherwise we run the risk of readers reading uninitialized memory.


	memset(r->_children, 0, n * sizeof(Child) + (n - 2) *
	sizeof(int));
	// mem fence here
	return r;

The reader trying to read the _v before the update on _v has been
committed, but the updater has changed the radix tree with the key
value.

This problem is also there in the case of multiple updaters accessing
the same location. 

If we have concurrent updates on _v, we can have a rollback mechanism
in place. Even if set is "false" we will insert the rule into the _v
table. If later we find that we have a "last_key" present we should
roll back the previous insert.


We can separate the locking betweeen the tree and _v using optimistic
insertion of routes in _v.

Problems which we have -- 
	 - Concurrent updates on _v -- we claim that can be solved
	   using optimistic insertion

	 - Concurrent updates on Radix
	 - Concurrent updates on _v and Radix
	 
	 - Concurrent update on _v with reader
	   can be solved with optimistic insertion

	 - Concurrent updates on Radix with reader
	 - Concurrent updates on _v and Radix and reader.


The deal with _default_key

Assume two concurrent updaters are tyring to update the table with a
/0 address. In that case, if there wasn't any previous /0 address or
we can overwrite, then one of them will win -- which is fine. But, if
say there was a reader along with these updaters, then the reader will
either read the preevious value or the current value and not something
in betweeen. Thus we don't need to do any locking.


Even though with RCU we can have lock free readers, we still need to
worry about updaters which might read stale data. This can be solved
by coarse grained locking in the updaters, but by careful design we
could get away with finer locks.

During an update, it's wrong to have:
       - A key is repeated on reclaim_now and/or reclaim_later.

       - The same key to be on reclaim_now and reclaim_later.
       	 This would mean that the updaters freed the same key
	 twice. For example, mark_as_free is defined as:

	 	mark_as_free(int key) {
		  _v[key].extra = _vfree;
		  _vfree = key;
		}

	If mark_as_free is called with the same key twice, before
	_vfree is used. We have a cycle. For example, consider key 1.
	
	_v[1].extra = _vfree;
	_vfree = 1;

	_v[1].extra = _vfree //_v[1].extra is now equal to 1
	_vfree = 1;
	
	Now when _vfree is used like: 
	found = _vfree //found = 1
	_vfree = _v[found].extra // _vfree = 1 , cycle


       - The same key to be on reclaim_now and on the _vfree list.
       	 This could potentially cause the updaters to use stale data. 
	 
       - The same key to be on reclaim_later and on the _vfree list.
       	 This could potentially cause the updaters to use stale data.

7/26/2011

	On running the test_10.click test, on the rcu branch, we
	observed that it's taking a lot more time than other
	tests. test_10.click has 4 writers and no readers. It has the
	rcu framework in place. We found that this runs much slower
	also in comparison to the case when there are 4 writers but
	doesn't use the RCU framwork (present in the rcu_master_tests
	branch).

	Our speculation is that the extreme degradation in performance
	of test_10.click is due to the reclaim operation. The reclaim
	operation needs the lock on the vector. This would not have
	been an issue if the reclaimation process didn't take much
	time, but due PoundRadixIPLookups nature (which does around
	10,000 updates per run_task call) the number of items which
	need to be reclaimed is large (around 40,000 items). All these
	items need to be added to the free list, which again take
	valuable time. Due to these reasons all the other threads are
	left waiting for the lock for a long time when even a single
	thread acquires the lock and performs the reclaimation.

